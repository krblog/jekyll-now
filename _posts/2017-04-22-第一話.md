---
layout: post
title: Adversarial Generator-Encoder Networks [arXiv:1704.02304]
category: 論文
tags:
- GAN
excerpt_separator: <!--more-->
---

## 第一話

- [Adversarial Generator-Encoder Networks](https://arxiv.org/abs/1704.02304)を読んだ時の話

<!--more-->

## 挨拶
G : 今週のヤンジャンは青山くんが風邪を引いていましたね。どうもguguchiです。

M : 今週のヤンジャンまだ買ってません。帰りに買います。masoです。

G : 今週は[Adversarial Generator-Encoder Networks](https://arxiv.org/abs/1704.02304)（以下、AGE）を読んでいきたいと思います。

リンクは後で修正します。

## 会話

G : まず概要について説明しますね。AGEはadversarial traingを使ってencoderとgeneratorを学習するという論文です。gをgenerator、eをencoderとすると、目標はgとeをadversarial trainingによって訓練する手法です。

$$ \max_e \min_g V(g,e) = KL [e(g(Z)) | e(X) ] $$

M : gとeをバトらせるんだよね

G：僕がよく分からなかったのは、なんでencoderについてmax取ってるかって所です。

M：２ぺージを見たら" Below, we show how to design an adversarial game between e and g that ensures the alignment of g(Z) = X in the data space, while only evaluating divergences in the latent space."って書いてあるね。目標は、潜在変数空間で二つの分布が最も離れるようにencoderを学習させるんですよね

G:いやそれはいいですけど、max minの順番はおかしくないですか？

M：僕もよく分からないです。最大最小不等式っていうのがありますよね。

$$\max_{z \in Z} \min_{w \in W} f(z,w) \leq \min_{w \in W} \max_{z \in Z} f(z,w)$$

これひっくり返すのって問題起こりそうですよね。でもアルゴリズム的には誰も気にしてませんですよね。交互にアップデートするし。

G:ひっくり返すのってやばいですよね。普通みんなmin maxでやってて、例えばGANだと

$$\min_G \max_D E_q[\log D(x)] + E_p[\log (1- D((Z))) ] := V(G,D) $$

こういう目的関数になっています。毎回やってるのは真の分布と生成モデルを最も離すような「距離」をdiscrimantorで学習して、それに基づいて生成モデルを真の分布に近付けるってことです。最終目標はV(G,D)=0にすることです。

M:100%同意っすよ。このmin maxってベクトル空間のinfinity norm最小化に似てますよね。

$$ d_{\infty}(p,q) = | p - q |{\infty} = \sup{k} {| p_k - q_k | } ~~~~ p, q \in V$$

qがベクトルでFがpを探す部分ベクトル空間だとしたら、

$$ \inf_{p \in F} d_{\infty}(p,q)= \inf_{p \in F} \sup_k{| p_k - q_k | } $$

を最小にするpを探したい。

G:kはベクトルの要素であってますよね？つまり、要素の差の中で最も大きい所が最も小さくなるようなpをFの中から探してくる。

M:そうだね！実はこれはもうちょっと拡張できる。

\begin{align} 
p_k - q_k = \langle p- q, e(k) \rangle 
\end{align}

e(k)はk番目の要素以外0のone-hotベクトルです。ここで取られているsupはone-hotベクトルの中から内積を取った時の最大となるone-hotベクトルを探してきている。

$$ \sup_k{|\langle p- q, e(k) \rangle| } $$

この話はもうちょっと拡張できて、one-hotベクトル以外のベクトルからも内積が最大となるものを探すってことができる。ベクトル空間V全体から探してくるとすると

\begin{align} 
\sup_{v \in V} {| \langle p- q, v \rangle| } = \sup_{v \in V} {\langle p, v \rangle - \langle q, v \rangle} 
\end{align}

となって、

$$ inf_{p \in F } \sup_{v \in V} {|\langle p, v \rangle - \langle q, v \rangle| } $$

を達成するp^*を探すことになる。

G:これってWasserstein GANの形に似てますね！

D：そうだね！積分は内積と考えることができて

$$\langle p, v\rangle = \int v(x) p(x) dx $$

pを確率分布とすると

$$\langle p, v\rangle = E_p(v(X)]$$ 

となるから,さっきの式は

$$ \inf_{p \in F } \sup_{v \in V} {|E_p[v(X)] - E_q[v(X)]\rangle| } $$

となる。Vを関数空間とするとWGANに完全に一致する。

G: うわーほんまやー！




















