---
layout: post
title: Boundary Equilibrium Generative Adversarial Networks [arXiv:1703.10717]
category: 論文
tags:
- GAN
excerpt_separator: <!--more-->
---

## This post is about... 
- a review of BEGAN https://arxiv.org/abs/1703.10717
<!--more-->
 
## Greetings! 

M: So, what episode was hot on Young jump this week? 

G: Huh?

M: Young jump!

G: Dude, it's golden week* ,man. There is no Young jump this week.  Hello everyone, this is Guguchi.

M: What a Bummer...  Hi, This is Maso!

*Golden week is a 'literal' cluster of 4 consecutive national holidays in the beginning of may. 
truly an Oasis for overworked Japanese. 

## BEGIN THE BEGAN! 

M: Well, let's begin the chat for today! Today's topic is BEGAN. It was a buzzword on the geek world for 
... how long? 

G: Ah, it came out on April, it's been exciting people for a month. I think the heat  
Well, How shall we describe BEGAN? 

M: In short, you mean. 

G: Yup.

M: Well, it is a method that aims to reduce Wasserstein distance between the reconstruction error of autoencoder for generated dataset and that of the autoencoder for real dataset.... at least that's what people say. 

G: Most importantly, it is a method that attempts to do away with min-max scheme. Its formulation is built to minimize a pair of objective function. The generated facial dataset is pretttty impressive. 

M: This paper bewildered me at first. So many parts of the formulation confused the hell out of me. 

G: Um, in fact, the Gaussian assumption of the reconstruction error took me off the guard, too. But why don't we go over the notations first? 
 
M: Sure thing!  Let's take a look at the equations in the paper.  First... the reconstruction error for the 
autoencoder $$D$$ with parameter $$\theta_D$$  is  given by 

$$ L(x; \theta_D) = \|x - D(x)\|_2$$ 

If $G$ is the generator, the goal is to make $$L(x; \theta_D)$$ approach 0 while keeping  $$ L(G(z); \theta_D) $$ and $$L(x; \theta_D)$$ as far apart as possible, in Wasserstein distance.

G: An then they insert something crazy here.  They assume that these are all Gaussians.  That is, 

$$ L(x; \theta_D) \sim  \mathcal{N}(m_1 , C_1) $$ 

$$ L(G(z) ; \theta_D) \sim  \mathcal{N}(m_2 , C_2) $$ 


M: In fact, they say "We found experimentally, for the datasets we tried, the loss distribution is, in fact, approximately normal."  The Wasserstein distance between two Wasserstein distance is given by:

$$W(\mu_1, \mu_2) = \|m_1 - m_2 \|_2^2  + trace(C_1 + C_2 - 2C_2^{1/2} C_1 C_2^{1/2})^{1/2}) $$ 

which in the case of dimension 1 is given by 


$$W(\mu_1, \mu_2) = \|m_1 - m_2 \|_2^2  + (c_1 + c_2 - 2 \sqrt{c_1 c_2})  $$ 





