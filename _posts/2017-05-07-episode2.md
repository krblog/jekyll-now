---
layout: post
title: Boundary Equilibrium Generative Adversarial Networks [arXiv:1703.10717]
category: 論文
tags:
- GAN
excerpt_separator: <!--more-->
---

## This post is about... 
- a review of BEGAN https://arxiv.org/abs/1703.10717
<!--more-->
 
## Greetings! 

M: So, what episode was hot on Young jump this week? 

G: Huh?

M: Young jump!

G: Dude, it's golden week* ,man. There is no Young jump this week. 

M: What a Bummer... 

*Golden week is a 'literal' cluster of 4 consecutive national holidays in the beginning of may. 
truly an Oasis for overworked Japanese. 

## BEGAN! 

M: Well, let's begin the chat for today! Today's topic is BEGAN. It was a buzzword on the geek world for 
... how long? 

G: Ah, it came out on April, it's been exciting people for a month. I think the heat  
Well, How shall we describe BEGAN? 

M: In short, you mean. 

G: Yup.

M: Well, it is a method that aims to Wasserstein distance between the reconstruction error of autoencoder for generated dataset and that of the autoencoder for real dataset.... at least that's what people say. 

G: Most importantly, it is a method that attempts to do away with min-max scheme. Its formulation is built to minimize a pair of objective function. The generated facial dataset is pretttty impressive. 

M: This paper bewildered me at first. So many parts of the formulation confused the hell out of me. 

G: Um, in fact, the Gaussian assumption of the reconstruction error took me off the guard, too. But why don't we go over the notations first? 
 
M: Sure thing!  Let's take a look at the equations in the paper.  First... the reconstruction error for the 
autoencoder $$D$$ with parameter $$\theta_D$$  is  given by 

$$ L(x; \theta_D) = \|x - D(x)\|_2$$ 




