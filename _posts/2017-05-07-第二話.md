---
layout: post
title: Boundary Equilibrium Generative Adversarial Networks(BEGAN) [arXiv:1703.10717]
category: 論文
tags:
- GAN
excerpt_separator: <!--more-->
---

## 第二話

- [BEGAN:Boundary Equilibrium Generative Adversarial Networks](https://arxiv.org/abs/1703.10717)を読んだ時の話

<!--more-->

## 挨拶
G : 今週のヤンジャンはGWのため、お休みでした。どうも泣きそうなguguchiです。

M : guguchiによると今週ヤンジャン休みみたいです。悔しいです。masoです。

G : 今週は1ヶ月くらい前に少しバズっていた[BEGAN:Boundary Equilibrium Generative Adversarial Networks](https://arxiv.org/abs/1703.10717)（以下、BEGAN）を読んでいきたいと思います。

## 会話

G : まず概要について説明します。BEGANは、discriminatorにauto-encoderを使って、生成モデルの再構成誤差の分布と真の分布の再構成誤差の分布のWasserstein距離を小さくしようという論文です。特徴は、従来のmin-maxの定式化ではなく、２つの目的関数を最小化するという定式化になっているところですね。論文中のリザルトの生成された顔画像は、エグいぐらいすごいリアリティーがありますね。

M : 俺ちょっとこの論文意味分かんないんだけど。。。

Y : masoさん、僕もめっちゃ意味わかんないとこありました。

M : やっぱり？とりあえず、論文中の式見ていきましょう！

Y : そうですね！まずは、discriminatorが$$\theta_D$$でパラメトライズされたauto-encoderになっていて、再構成誤差は

$$ L(x; \theta_D) = \|x - D(x)\|_2$$ 

で計算されます。
ここで一つの仮定が入ります。それは、再構成誤差の分布が$$ L(x; \theta_D) \sim  \mathcal{N}(m , C) $$になるという仮定です。
論文中では、"We found experimentally, for the datasets we tried, the loss distribution is, in fact, approximately normal."って言ってますね。

M : もしGがgeneratorなら、この手法の$$\theta_D$$に関する目的は、$$ L(x; \theta_D) $$は0に近づける。一方で$$ L(G(z); \theta_D)$$と$$ L(x; \theta_D) $$のWasserstein距離をに関して遠ざけることだね。



