---
layout: post
title: Boundary Equilibrium Generative Adversarial Networks(BEGAN) [arXiv:1703.10717]
category: 論文
tags:
- GAN
excerpt_separator: <!--more-->
---

## 第二話

- [BEGAN:Boundary Equilibrium Generative Adversarial Networks](https://arxiv.org/abs/1703.10717)を読んだ時の話

<!--more-->

## 挨拶
G : 今週のヤンジャンはGWのため、お休みでした。どうも泣きそうなguguchiです。

M : guguchiによると今週ヤンジャン休みみたいです。悔しいです。masoです。

G : 今週は1ヶ月くらい前に少しバズっていた[BEGAN:Boundary Equilibrium Generative Adversarial Networks](https://arxiv.org/abs/1703.10717)（以下、BEGAN）を読んでいきたいと思います。

## 会話

G : まず概要について説明します。BEGANは、discriminatorにauto-encoderを使って、生成モデルの再構成誤差の分布と真の分布の再構成誤差の分布のWasserstein距離を小さくしようという論文です。特徴は、従来のmin-maxの定式化ではなく、２つの目的関数を最小化するという定式化になっているところですね。論文中のリザルトの生成された顔画像は、エグいぐらいすごいリアリティーがありますね。

M : 俺ちょっとこの論文意味分かんないんだけど。。。

Y : masoさん、僕もめっちゃ意味わかんないとこありました。

M : やっぱり？とりあえず、論文中の式見ていきましょう！

Y : そうですね！まずは、discriminatorが$$\theta_D$$でパラメトライズされたauto-encoderになっていて、再構成誤差は

$$ L(x; \theta_D) = \|x - D(x)\|_2$$ 

で計算されます。

M : もしGがgeneratorなら、この手法のdiscriminatorの目的は、$$ L(x; \theta_D) $$は0に近づける。一方で$$ L(G(z); \theta_D)$$と$$ L(x; \theta_D) $$のWasserstein距離を遠ざけることだね。

Y : そうです。ここで一つの仮定が入ります。それは、真の分布・生成モデルの再構成誤差の分布がそれぞれ

$$ L(x; \theta_D) \sim  \mathcal{N}(m_1 , C_1) $$ 

$$ L(G(z) ; \theta_D) \sim  \mathcal{N}(m_2 , C_2) $$ 

になるという仮定です。
論文中では、"We found experimentally, for the datasets we tried, the loss distribution is, in fact, approximately normal."って言ってますね。
この仮定を使うと、ガウシアンのWasserstein距離が解析的に計算できるという性質が使えます。すると

$$W(\mu_1, \mu_2) = \|m_1 - m_2 \|_2^2  + trace(C_1 + C_2 - 2C_2^{1/2} C_1 C_2^{1/2})^{1/2}) $$ 

が導けます。さらに次元が１次元の場合は、

$$W(\mu_1, \mu_2) = \|m_1 - m_2 \|_2^2  + (c_1 + c_2 - 2 \sqrt{c_1 c_2})  $$ 

で計算できます。

